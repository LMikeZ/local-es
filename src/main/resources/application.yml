server:
  context-path: /local-es
  port: 8007

spring:
  application:
    name: local-es
  datasource:
    username: root
    password: root
    url: jdbc:mysql://192.168.32.128:3306/test?autoReconnect=true&failOverReadOnly=false&maxReconnects=10&characterEncoding=UTF8&allowMultiQueries=true&useSSL=false
    type: com.alibaba.druid.pool.DruidDataSource
    driver-class-name: com.mysql.cj.jdbc.Driver
  redis:
    database: 0
    password: root
    pool:
      max-active: 50
      max-idle: 20
    host: 192.168.32.128

  kafka:
    bootstrap-servers: 10.0.10.158:19092,10.0.10.158:29092,10.0.10.158:39092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      acks: 1
    consumer:
      group-id: uti-flow-message #群组ID
      enable-auto-commit: false #true 自动提交处理到哪个offset，false 手动提交
      # auto-commit-interval: 1000 #多久提交一次offset
      auto-offset-reset: latest
      #earliest当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
      #latest当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
      #nonetopic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    listener:
      ack-mode: MANUAL_IMMEDIATE

elasticsearch:
  host: 192.168.32.128:9200
mybatis-plus:
  config-location: classpath:mybatis-config.xml
